# -*- coding: utf-8 -*-
"""B20EE073_Prml_Bonus_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Znc5nLe9EH5csbMwoEdNj_FcLF2_Iud0
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plot
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import seaborn as sns 
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from lightgbm import LGBMRegressor
from sklearn import svm
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import GradientBoostingRegressor
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

from google.colab import drive
drive.mount('/content/drive')

data=pd.read_csv('/content/drive/MyDrive/BitcoinDataset.csv',index_col='Date',parse_dates=True)

data

# converting the string to datetime format
#data['Date'] = pd.to_datetime(data['Date'], infer_datetime_format=True)

data.shape

data.describe()

data.isna().sum()

data.info()

data.corr()

def corrl(data,threshold):
  corr=data.corr()['Market Cap'].sort_values(ascending=False)[1:]
  abs_corr=abs(corr)
  relevant_features=abs_corr[abs_corr>threshold]
  return relevant_features

#corr_features=corrl(data,0.81)

dat3=data.copy()

dff=data.iloc[:,[0,1,2,3]]

dff



"""#Visualization"""

data['Close'].plot(figsize=(12,8),color='red')
plot.show()

data['2017']['Close'].plot(figsize=(10,6))

data['2017'].plot(figsize=(12,8))

data['2012':'2014']['Close'].plot(figsize=(12,8),color='green')

data['2015':'2017']['Close'].plot(figsize=(12,8),color='orange')

data['2012':'2017']['Close'].plot(figsize=(12,8),color='purple')

data.loc['2012':'2017']['Close'].resample('M').plot(figsize=(16,10))
plot.show()

data.loc['2012':'2017']['Close'].resample('M').mean().plot.bar(figsize=(20,12))
plot.show()



"""#Split the dataset into train and test sets"""

from sklearn.model_selection import train_test_split

X = dff.iloc[:, 0:-1].values
Y = data['Close'].values

X_train, X_test, Y_train,Y_test = train_test_split(X, Y, test_size=.3, random_state=15)

X_train.shape, X_test.shape, Y_train.shape,Y_test.shape

dff

"""#Creating pipeline and testing the various models"""

from sklearn.pipeline import Pipeline

pipeline_lr=Pipeline([('scalar1',StandardScaler()),('lr_regressor',LogisticRegression(random_state=0))])

pipeline_dt=Pipeline([('scalar2',StandardScaler()),('dt_regressor',DecisionTreeRegressor(max_depth=22))])

pipeline_randomforest=Pipeline([('scalar3',StandardScaler()),('rf_regressor',RandomForestRegressor(n_estimators=700,max_depth=10))])

pipeline_lgbm=Pipeline([('scalar4',StandardScaler()),('lgbm_regressor',LGBMRegressor(n_estimators=2000, eta=0.5,max_depth=3))])

pipeline_knn=Pipeline([('scalar5',StandardScaler()),('knn_model',KNeighborsRegressor(n_neighbors=3))])

pipeline_mlpRegressor=Pipeline([('scalar7',StandardScaler()),('mlp_regressor',MLPRegressor(hidden_layer_sizes=(8,8,8), activation='relu', solver='adam', max_iter=20))])

pipeline_gradient_boosting_regressor=Pipeline([('scalar8',StandardScaler()),('gradient_boosting_regressor',GradientBoostingRegressor(n_estimators=20, learning_rate=0.1,max_depth=1, random_state=0))])

## Lets make the list of pipelines
pipelines = [pipeline_lr, pipeline_dt, pipeline_randomforest,pipeline_lgbm,pipeline_mlpRegressor,pipeline_gradient_boosting_regressor]

best_accuracy=0.0
best_classifier=0
best_pipeline=""

X_train, Y_train

# Dictionary of pipelines and classifier types for ease of reference
pipe_dict = {1: 'Logistic Regression', 2: 'Decision Tree', 3: 'RandomForest', 4: 'Lightgbm', 5: 'MLP', 6: 'Gradient Boosting'}

# Fit the pipelines
for pipe in pipelines:
	pipe.fit(X_train.astype(int), Y_train.astype(int))

for i in pipe_dict:
  print(pipe_dict[i])

l2=[]
for i,model in zip(pipe_dict,pipelines):
  l2.append(model.predict(X_test))
  print()
  print("Predicted values:",pipe_dict[i],'=')
  print(model.predict(X_test))

def plot(predict,Y_test):
  plot.figure(figsize=(15, 8))
  test_day = [t for t in range(len(Y_test))]
  labels={'Orginal','Predicted'}
  plot.plot(test_day, predict, color= 'black')
  plot.plot(test_day, Y_test, color = 'red')
  plot.title('Expected Vs Predicted Views Forecasting')
  plot.ylabel('Closing Price')
  plot.legend(labels)
  plot.show()

l2=[]
l3=[]
for i,model in zip(pipe_dict,pipelines):
  for j in range(len(X_test)):
    m = model.predict(X_test)
    error=((abs(Y_test[j]-m[j]))/Y_test[j])*100
    print('model:', pipe_dict[i],'=')
    print('Expected value =',Y_test[j],'-------predicted value =',m[j],'-----------error = ',"{:.2f}".format(error),'%')
    l3=m
    l2=error
  plot.figure(figsize=(15, 8))
  test_day = [t for t in range(len(Y_test))]
  labels={'Orginal','Predicted'}
  plot.plot(test_day, l3, color= 'black')
  plot.plot(test_day, Y_test, color = 'green')
  plot.title('Expected Vs Predicted Views Forecasting')
  plot.ylabel('Closing Price')
  plot.legend(labels)
  plot.show()
  print('error=',np.array(error).mean())
  print